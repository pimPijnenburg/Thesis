{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "train_df = pd.read_parquet('/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/train_df.parquet')\n",
    "val_df = pd.read_parquet('/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/val_df.parquet')\n",
    "test_df = pd.read_parquet('/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset:\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n",
      "\n",
      "Validation Dataset:\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n",
      "\n",
      "Test Dataset:\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n",
      "Expected min of 0 and max of 255\n"
     ]
    }
   ],
   "source": [
    "#Finding out the min and max values for normalization purposes\n",
    "for df_name, df in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"\\n{df_name} Dataset:\")\n",
    "\n",
    "    for channel in ['red_channel', 'green_channel', 'blue_channel']:\n",
    "        channel_data = np.concatenate(df[channel].values)\n",
    "        \n",
    "        min_val = channel_data.min()\n",
    "        max_val = channel_data.max()\n",
    "        \n",
    "        assert min_val == 0\n",
    "        assert max_val == 255\n",
    "        \n",
    "        print('Expected min of 0 and max of 255')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the color channels, combines them into an images, reshapes it to 256x256x3, normalizes, and one-hot encodes the data\n",
    "def preprocess_image(red, green, blue, label): \n",
    "    image = tf.stack([red, green, blue], axis = -1)\n",
    "    \n",
    "    #Reshape and normalize the data\n",
    "    image = tf.reshape(image, (256, 256, 3))\n",
    "    image = tf.cast(image, tf.float16) / 255.0\n",
    "    \n",
    "    #One-hot encoding\n",
    "    label = tf.one_hot(label, depth = 6)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a TensorFlow dataset from the created dataframes. Applies the preprocessing function to the dataframes. \n",
    "#Batches are created that be used for training the model later on\n",
    "\n",
    "def create_dataset(df, batch_size = 32, shuffle = True): \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        df['red_channel'].tolist(), \n",
    "        df['green_channel'].tolist(), \n",
    "        df['blue_channel'].tolist(), \n",
    "        df['class'].tolist()\n",
    "    ))\n",
    "    \n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls= tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(buffer_size= len(df))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating of the datasets incorporating the functions above \n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df, shuffle=False)\n",
    "test_dataset = create_dataset(test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data for later usage\n",
    "tf.data.Dataset.save(train_dataset, '/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/for_training/train')\n",
    "tf.data.Dataset.save(val_dataset, '/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/for_training/val')\n",
    "tf.data.Dataset.save(test_dataset, '/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS Dataset/data/created_data/for_training/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
