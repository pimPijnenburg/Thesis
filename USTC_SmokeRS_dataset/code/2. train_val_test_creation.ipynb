{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "class_mapping = str({'Cloud': 0, 'Dust': 1, 'Haze': 2, 'Land': 3, 'Seaside': 4, 'Smoke': 5})\n",
    "\n",
    "df = pd.read_parquet('/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS_dataset/data/created_data/smoke_data/smoke_data.parquet')\n",
    "display(df.head())\n",
    "print(f'\\n--- CLASS MAPPING --- \\n{class_mapping}\\n\\n')\n",
    "print(f'--- DF SHAPE BEFORE AUGMENTATION --- \\n{df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df):\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        original_image = np.stack([row['red_channel'], row['green_channel'], row['blue_channel']], axis=-1)\n",
    "        original_image = original_image.reshape((256, 256, 3))\n",
    "        \n",
    "        # Original image\n",
    "        augmented_data.append({\n",
    "            'red_channel': original_image[:,:,0].flatten(),\n",
    "            'green_channel': original_image[:,:,1].flatten(),\n",
    "            'blue_channel': original_image[:,:,2].flatten(),\n",
    "            'class': row['class']\n",
    "        })\n",
    "        \n",
    "        # Mirrored image\n",
    "        mirrored = np.fliplr(original_image)\n",
    "        augmented_data.append({\n",
    "            'red_channel': mirrored[:,:,0].flatten(),\n",
    "            'green_channel': mirrored[:,:,1].flatten(),\n",
    "            'blue_channel': mirrored[:,:,2].flatten(),\n",
    "            'class': row['class']\n",
    "        })\n",
    "        \n",
    "        # Rotations of mirrored image (90, 180, 270 degrees)\n",
    "        for k in [1, 2, 3]:  # 90, 180, 270 degrees\n",
    "            rotated = np.rot90(mirrored, k=k)\n",
    "            augmented_data.append({\n",
    "                'red_channel': rotated[:,:,0].flatten(),\n",
    "                'green_channel': rotated[:,:,1].flatten(),\n",
    "                'blue_channel': rotated[:,:,2].flatten(),\n",
    "                'class': row['class']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_df = augment_data(df)\n",
    "#print(f'\\n--- DF SHAPE AFTER AUGMENTATION --- \\n{augmented_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "total_size = df.shape[0]\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = (total_size - train_size) // 2\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Verify that all samples are accounted for \n",
    "assert train_size + val_size + test_size == total_size\n",
    "#print('Full augmented dataframe size utilized.') \n",
    "\n",
    "X = df[['red_channel','green_channel','blue_channel']]\n",
    "y = df['class']\n",
    "\n",
    "# First split: Creating X_train\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=train_size, \n",
    "                                                            shuffle=True, stratify=y, random_state=1)\n",
    "\n",
    "# Second split: create X_val and X_test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, train_size=val_size, \n",
    "                                                shuffle=True, stratify=y_val_test, random_state=1)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Assert that the DF's shapes add up\n",
    "assert train_df.shape[0] + val_df.shape[0] + test_df.shape[0] == df.shape[0]\n",
    "#print('Dataframes\\' sizes add up to total augmented df size.')\n",
    "\n",
    "# Assert no overlap between train, val and test set\n",
    "train_idx = set(train_df.index)\n",
    "val_idx = set(val_df.index)\n",
    "test_idx = set(test_df.index)\n",
    "\n",
    "assert len(train_idx.intersection(val_idx)) == 0\n",
    "assert len(train_idx.intersection(test_idx)) == 0\n",
    "assert len(val_idx.intersection(test_idx)) == 0\n",
    "\n",
    "# Assert that all augmented indices are accounted for\n",
    "all_split_indices = train_idx.union(val_idx).union(test_idx)\n",
    "assert len(all_split_indices) == df.shape[0]\n",
    "\n",
    "print(\"No index overlap detected.\")\n",
    "print()\n",
    "print('--- TRAIN - VAL - TEST SIZES ---')\n",
    "print(f'Train size: {train_df.shape[0]}')\n",
    "print(f'Validation size: {val_df.shape[0]}')\n",
    "print(f'Test size: {test_df.shape[0]}')\n",
    "\n",
    "\n",
    "path = '/Users/pimpijnenburg/Desktop/Thesis/USTC_SmokeRS_dataset/data/created_data/train_val_test_split'\n",
    "\n",
    "train_df.to_parquet(os.path.join(path, 'train.parquet'))\n",
    "val_df.to_parquet(os.path.join(path, 'val.parquet'))\n",
    "test_df.to_parquet(os.path.join(path, 'test.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
