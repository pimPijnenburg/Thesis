{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "from transformers import TFConvNextV2ForImageClassification, ConvNextV2Config\n",
    "from tl_tools import *\n",
    "\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 17:42:58.847528: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-10-05 17:42:58.847547: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-10-05 17:42:58.847552: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-10-05 17:42:58.847564: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-10-05 17:42:58.847575: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train, val, test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXtV2\n",
    "Running into some problems here. Maybe have to use PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 17:46:24.515612: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : NOT_FOUND: could not find registered platform with id: 0x15d2ac9f0\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Exception encountered when calling layer 'dwconv' (type Conv2D).\n\ncould not find registered platform with id: 0x15d2ac9f0 [Op:__inference__jit_compiled_convolution_op_10293]\n\nCall arguments received by layer 'dwconv' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 64, 64, 96), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m ConvNextV2Config(num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m convnextv2 \u001b[38;5;241m=\u001b[39m TFConvNextV2ForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/convnextv2-base-22k-224\u001b[39m\u001b[38;5;124m'\u001b[39m,config\u001b[38;5;241m=\u001b[39mconfig,from_pt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Freeze the base model if needed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m convnextv2\u001b[38;5;241m.\u001b[39mconvnextv2\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2963\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[1;32m   2962\u001b[0m     \u001b[38;5;66;03m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[0;32m-> 2963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_pytorch_checkpoint_in_tf2_model(\n\u001b[1;32m   2964\u001b[0m         model,\n\u001b[1;32m   2965\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2966\u001b[0m         allow_missing_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2967\u001b[0m         output_loading_info\u001b[38;5;241m=\u001b[39moutput_loading_info,\n\u001b[1;32m   2968\u001b[0m         _prefix\u001b[38;5;241m=\u001b[39mload_weight_prefix,\n\u001b[1;32m   2969\u001b[0m         tf_to_pt_weight_rename\u001b[38;5;241m=\u001b[39mtf_to_pt_weight_rename,\n\u001b[1;32m   2970\u001b[0m     )\n\u001b[1;32m   2972\u001b[0m \u001b[38;5;66;03m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_weight_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_pytorch_utils.py:211\u001b[0m, in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[0;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[1;32m    207\u001b[0m     pt_state_dict\u001b[38;5;241m.\u001b[39mupdate(state_dict)\n\u001b[1;32m    209\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch checkpoint contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(t\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mpt_state_dict\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_pytorch_weights_in_tf2_model(\n\u001b[1;32m    212\u001b[0m     tf_model,\n\u001b[1;32m    213\u001b[0m     pt_state_dict,\n\u001b[1;32m    214\u001b[0m     tf_inputs\u001b[38;5;241m=\u001b[39mtf_inputs,\n\u001b[1;32m    215\u001b[0m     allow_missing_keys\u001b[38;5;241m=\u001b[39mallow_missing_keys,\n\u001b[1;32m    216\u001b[0m     output_loading_info\u001b[38;5;241m=\u001b[39moutput_loading_info,\n\u001b[1;32m    217\u001b[0m     _prefix\u001b[38;5;241m=\u001b[39m_prefix,\n\u001b[1;32m    218\u001b[0m     tf_to_pt_weight_rename\u001b[38;5;241m=\u001b[39mtf_to_pt_weight_rename,\n\u001b[1;32m    219\u001b[0m )\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_pytorch_utils.py:255\u001b[0m, in \u001b[0;36mload_pytorch_weights_in_tf2_model\u001b[0;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Numpy doesn't understand bfloat16, so upcast to a dtype that doesn't lose precision\u001b[39;00m\n\u001b[1;32m    252\u001b[0m pt_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    253\u001b[0m     k: v\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;28;01melse\u001b[39;00m v\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pt_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    254\u001b[0m }\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_pytorch_state_dict_in_tf2_model(\n\u001b[1;32m    256\u001b[0m     tf_model,\n\u001b[1;32m    257\u001b[0m     pt_state_dict,\n\u001b[1;32m    258\u001b[0m     tf_inputs\u001b[38;5;241m=\u001b[39mtf_inputs,\n\u001b[1;32m    259\u001b[0m     allow_missing_keys\u001b[38;5;241m=\u001b[39mallow_missing_keys,\n\u001b[1;32m    260\u001b[0m     output_loading_info\u001b[38;5;241m=\u001b[39moutput_loading_info,\n\u001b[1;32m    261\u001b[0m     _prefix\u001b[38;5;241m=\u001b[39m_prefix,\n\u001b[1;32m    262\u001b[0m     tf_to_pt_weight_rename\u001b[38;5;241m=\u001b[39mtf_to_pt_weight_rename,\n\u001b[1;32m    263\u001b[0m )\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_pytorch_utils.py:329\u001b[0m, in \u001b[0;36mload_pytorch_state_dict_in_tf2_model\u001b[0;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inputs:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(_prefix):\n\u001b[0;32m--> 329\u001b[0m         tf_model(tf_inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Make sure model is built\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Convert old format to new format if needed from a PyTorch state_dict\u001b[39;00m\n\u001b[1;32m    331\u001b[0m tf_keys_to_pt_keys \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/models/convnextv2/modeling_tf_convnextv2.py:649\u001b[0m, in \u001b[0;36mTFConvNextV2ForImageClassification.call\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 649\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvnextv2(\n\u001b[1;32m    650\u001b[0m     pixel_values,\n\u001b[1;32m    651\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    652\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    653\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    658\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/models/convnextv2/modeling_tf_convnextv2.py:428\u001b[0m, in \u001b[0;36mTFConvNextV2MainLayer.call\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    426\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m--> 428\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    429\u001b[0m     embedding_output,\n\u001b[1;32m    430\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    431\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    432\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# Change to NCHW output format have uniformity in the modules\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/models/convnextv2/modeling_tf_convnextv2.py:379\u001b[0m, in \u001b[0;36mTFConvNextV2Encoder.call\u001b[0;34m(self, hidden_states, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    377\u001b[0m         all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_module(hidden_states)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    382\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/models/convnextv2/modeling_tf_convnextv2.py:327\u001b[0m, in \u001b[0;36mTFConvNextV2Stage.call\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    325\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer(hidden_states)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 327\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer(hidden_states)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/tensorflow-test/env/lib/python3.11/site-packages/transformers/models/convnextv2/modeling_tf_convnextv2.py:223\u001b[0m, in \u001b[0;36mTFConvNextV2Layer.call\u001b[0;34m(self, hidden_states, training)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 223\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdwconv(hidden_states)\n\u001b[1;32m    224\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(x)\n\u001b[1;32m    225\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwconv1(x)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Exception encountered when calling layer 'dwconv' (type Conv2D).\n\ncould not find registered platform with id: 0x15d2ac9f0 [Op:__inference__jit_compiled_convolution_op_10293]\n\nCall arguments received by layer 'dwconv' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 64, 64, 96), dtype=float32)"
     ]
    }
   ],
   "source": [
    "config = ConvNextV2Config(num_labels = 6, image_size= 256)\n",
    "convnextv2 = TFConvNextV2ForImageClassification.from_pretrained('facebook/convnextv2-base-22k-224',config=config,from_pt=True)\n",
    "\n",
    "# Freeze the base model if needed\n",
    "convnextv2.convnextv2.trainable = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
